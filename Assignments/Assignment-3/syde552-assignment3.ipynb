{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SYDE 552 Assignment 3: Hippocampal Models\n",
    "### Due Monday, March, Anywhere on Earth \n",
    "### Value: 15% of total marks for the course\n",
    "\n",
    "The purpose of this assigment is to give you experience working with associative memories. To do so, we'll be using pytorch to implement different associative memory models.\n",
    "\n",
    "You can work in groups to do the assignment, but your answers and code must be original to you. Your submission will be a filled-out copy of this notebook (cells for code and written answers provided)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. The Hippocampus\n",
    "\n",
    "**1.a) [2 marks]** The hippocampus is implicated in spatial navigation and episodic memory.  How do we know this?  What are some of the neuroscience results that revealed these facilities of the hippocampus?  (see Kandel *et al.* Ch. 65)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.b) [2 marks]** The Gluck and Meyers model of hippocampus is a simple, effective model of hippocampus, and how representations may be constructed for the slow transfer to the neocortex.  However, it still has its limitations.  Explain some (two or more) of the limitations of the Gluck and Meyers model.  (The Gluck and Meyers Ch.6 reading will be useful in answering this question.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.c) [2 mark]** Unsupervised pre-training is integral to the Gluck and Meyers model of Hippocampus, and unsupervised pre-training has been shown to accelerate reinforcement learning in rats navigating mazes.  However, unsupervised pre-training is not common in deep learning techniques.  Describe why that may be.  (See [$\\S 15.1$ of Goodfellow *et al.*](https://www.deeplearningbook.org/contents/representation.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Hopfield Networks\n",
    "\n",
    "## Preliminaries\n",
    "\n",
    "Although you should have installed them for prior assignments, we will require the pytorch and keras. Let's install those now/\n",
    "\n",
    "```pip install torch keras```\n",
    "\n",
    "Next, we will download the MNIST dataset.  We will do this through the Keras library instead of torchvision.\n",
    "\n",
    "Some people have been having problems with the code in the next block due to certificate errors.  If you do have this problem, try using the following code:\n",
    "\n",
    "```python\n",
    "import ssl\n",
    "import keras.datasets.mnist\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "```\n",
    "\n",
    "I don't recommend doing this if you don't have too - ignoring certificate errors is bad security practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we will examine some of the images to make sure we got them right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(14,8))\n",
    "for i in range(24):\n",
    "    plt.subplot(4, 6, i+1)\n",
    "    plt.imshow(x_train[i].reshape((28,28)), vmin=0, vmax=255, cmap='gray_r')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.title(int(y_train[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally, the MNIST dataset has images represented by values in the range $[0,255]$. However, since we are dealing with Hopfield networks, we are going to binarize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize(xs):\n",
    "    '''\n",
    "    xs : a num_samples by num_features array of images.\n",
    "    '''\n",
    "    binary = (xs / 255) > 0.5\n",
    "    integer = 2 * binary - 1\n",
    "    return integer\n",
    "\n",
    "binary_imgs = binarize(x_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further, to make things easy on our network, we are only going to look at images of the digits 0, 1, and 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_imgs = binary_imgs[y_train < 3,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,8))\n",
    "for i in range(24):\n",
    "    plt.subplot(4, 6, i+1)\n",
    "    plt.imshow(binary_imgs[i].reshape((28,28)), vmin=-1, vmax=1, cmap='gray_r')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.title(int(y_train[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also need to reshape the data into a vector representation, and then covert it to a pytorch tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_img_vecs = binary_imgs.reshape((-1,28*28))\n",
    "binary_img_tensor = torch.from_numpy(binary_img_vecs).to(torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**2.a) [2 Marks]**  Implement the Hopfield Network using Hopfield's learning rule.\n",
    "\n",
    "We will first implement the Hopfield network using the outer product formulation for the weight matrix.  Given a set of patterns $\\mathbf{x}_{i}$, by first computing the matrix\n",
    "\n",
    "$$D = \\frac{1}{N}\\sum_{i}^{N} (\\mathbf{x}_{i}-\\theta)(\\mathbf{x}_{i}-\\theta)^T$$.\n",
    "\n",
    "where $\\theta = \\frac{1}{ND}\\sum_{i}^{N}\\sum_{j}^{D} x_{i,j}$, i.e., the average value of all elements in the training data.\n",
    "\n",
    "Next we remove the diagonal element of the matrix, making the weight matrix:\n",
    "\n",
    "$$\n",
    "W = D - \\mathrm{diag}(D),\n",
    "$$\n",
    "\n",
    "where $\\mathrm{diag}(D)$ is the diagonal of the $D$ matrix.  Note that when implementing this in numpy or pytorch we must apply the ```diag``` function twice, i.e.:\n",
    "\n",
    "```\n",
    "W = D - torch.diag(torch.diag(D))\n",
    "\n",
    "```\n",
    "\n",
    "Here is some code to implement the learning rule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def outer_product_hopfield_matrix(training_patterns):\n",
    "    '''\n",
    "    train_hopfield_matrix - produces a matrix for a (non-Modern) Hopfield network using the outer product rule.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    \n",
    "    training_patterns : torch.Tensor\n",
    "    \n",
    "        A Tensor of shape (num_patterns, num_features) that will be used to construct the weight matrix.\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    \n",
    "    W : torch.Tensor\n",
    "    \n",
    "        A (num_features, num_features) Tensor that stores the papers encoded in the network.\n",
    "    '''\n",
    "    \n",
    "    theta = torch.sum(training_patterns) / (training_patterns.shape[0] * training_patterns.shape[1])\n",
    "    D = torch.einsum('nd,ne->de', training_patterns - theta, training_patterns - theta) / float(training_patterns.shape[0])\n",
    "    W = (D - torch.diag(torch.diag(D))) \n",
    "    return W\n",
    "    \n",
    "def evaluate_hopfield_network(W, input_pattern, training_patterns=None, num_iters = 5, threshold=0):\n",
    "    '''\n",
    "    Evaluates a Hopfield network with weight matrix W on a number of tets patterns.  Also computed the similarity \n",
    "    '''\n",
    "    \n",
    "    assert input_pattern.shape[0] == 1, f'''This function assumes you are cleaning up one pattern at a time. \n",
    "                                            Expected the input to be shape (1,{input_pattern.shape[1]}), \n",
    "                                            got {input_pattern.shape}'''\n",
    "    s = input_pattern\n",
    "    \n",
    "    similarities = None\n",
    "    if training_patterns is not None:\n",
    "        similarities = torch.zeros((num_iters, training_patterns.shape[0]))\n",
    "    ### end if\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        s = torch.sign(W @ s - threshold)\n",
    "        \n",
    "        if training_patterns is not None:\n",
    "            similarities[i,:] = torch.einsum('d,nd->n',s,training_patterns)\n",
    "        ### end if\n",
    "    ### end for\n",
    "    return s, similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want you to do the following things:\n",
    "\n",
    "1. Compute the capacity of the network, using the expression $C \\approx \\frac{d}{2\\log_{2}(d)}$, where $d$ is the number of neurons.\n",
    "2. For a training set of 10 patterns, plot the original image and the reconstructed image side-by-side.\n",
    "3. Plot the training and test error (Mean squared error between the predicted and true values) of the Hopfield network as a function of the number of patterns stored in the network up to capacity, $C$, for five randomly selected training sets selected from ```binary_img_tensor```.\n",
    "4. For a training set of size 10 patterns, add salt and pepper noise (bit flips) to the **training** images and compute the training error as the probability of noise increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2.a.1 - Compute network capacity.\n",
    "C = # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.a.2 - plot original and reconstructed imatges.\n",
    "plt.figure()\n",
    "# create weight matrix, W\n",
    "for i in range(10):\n",
    "    # reconstruct training image, i.\n",
    "    plt.subplot(10,2,1+2*i)\n",
    "    # plot original image \n",
    "    # [your code here]\n",
    "    plt.subplot(10,2,1+2*i+1)\n",
    "    # plot reconstructed image.\n",
    "    # [your code here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.a.3 - Plot training and test error\n",
    "\n",
    "def mse(true_vals, pred_val):\n",
    "    return ### your code here\n",
    "\n",
    "def test_network(training_pattens, test_patterns, C):\n",
    "    assert training_patterns.shape[0] == C, f'''Error: Expected training patterns to contain {C} patterns'''\n",
    "    training_error = torch.zeros((len(range(2,C),))\n",
    "    test_error = torch.zeros((len(range(2,C),))\n",
    "    for pattern_idx, num_patterns in enumerate(range(2,C)):\n",
    "        # train network\n",
    "        ## NOTE: you will need to change the following line if you want to use something other\n",
    "        ## than the outer product matrix.\n",
    "        W = outer_product_hopfield_matrix(binary_img_tensor[:num_patterns,:])\n",
    "        # for each image in the training set, compute the mse\n",
    "    \n",
    "    \n",
    "        for test_idx in range(num_patterns):\n",
    "            pass ### your code here\n",
    "        ### end for\n",
    "        results[pattern_idx] = None # your code here\n",
    "    ### end for\n",
    "    return training_error, test_error\n",
    "\n",
    "\n",
    "average_results = None # your code here.\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(2,C), average_results )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.a.4 - Test salt-and-pepper noise.\n",
    "\n",
    "num_steps = None ### Pick a value\n",
    "prob_flip = torch.linspace(0.01,0.5,num_steps)\n",
    "\n",
    "training_data = # Your code here\n",
    "testing_data = # Your code here\n",
    "W = # Your code here\n",
    "\n",
    "for p_idx, p in enumerate(prob_flip):\n",
    "    ## Your code here\n",
    "    pass\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(prob_flip, mses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.b) [2 Mark]** Train Hopfield network using the pseudo-inverse and repeat the tasks 2.a) 2-4, however, for this example, ensure the threshold value is 0.  Even though the pseudo-inverse has a greater capacity thant he hopfield network, we will still test up to the theoretical capcity of the Hopfield learning rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pseudo_inv(X, lamb=0.01):\n",
    "    '''\n",
    "    pseudo_inv - Implements the pseudoinverse from the previous assignment.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    \n",
    "    X : torch.Tensor\n",
    "        A (num_patterns, num_features) Tensor holding the training data.\n",
    "        \n",
    "    lamb : float\n",
    "        The regularization term for the pseudoinverse\n",
    "        \n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    \n",
    "    The pseudoinverse of X\n",
    "    '''\n",
    "    return torch.inverse(X.T@X + lamb * torch.eye(X.shape[1]).float()) @ X.T\n",
    "\n",
    "def pseudoinverse_hopfield_matrix(training_patterns, lamb = 0.01):\n",
    "    '''\n",
    "    pseudoinverse_hopfield_matrix - Uses the regularized pseudoinverse to construct a weight matrix for\n",
    "        a non-modern Hopfield network.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    \n",
    "    training_patterns : torch.Tensor\n",
    "        \n",
    "        The (num_patterns, num_features) Tensor containing the training data.\n",
    "        \n",
    "    lamb : float\n",
    "    \n",
    "        The regularization term for the pseudo-inverse. \n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    \n",
    "        The weight matrix compatible with the evaluate_hopfield_network function \n",
    "    '''\n",
    "    W = pseudo_inv(training_patterns, lamb=lamb) @ training_patterns\n",
    "    return W / training_patterns.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.c) [1 Mark]** Show how the network behaves when an entire region of the image is corrupted. \n",
    "\n",
    "For your best performing network, take three test images and set half of the inputs to be equal to zero.  \n",
    "Plot the original images, the corrupted images, and the reconstructed images, side-by-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Autoencoders\n",
    "\n",
    "Now we are going to train an autoencoder to perform the same associative task that we explored above.  We are going to define our Autoencoder using sigmoid neurons, which shouldn't be too terrible, since it is a relatively shallow network, but one is not obligated to use those. \n",
    "\n",
    "Because we are going to use a sigmoidal output function, so we should be concerned with values in the range $[0,1]$, instead of $\\{-1,1\\}$.  Training will be done using the original MNIST images, **not** the binary images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "x_train_tensor = torch.from_numpy(x_train.reshape((-1,28*28) / 255))\n",
    "x_test_tensor = torch.from_numpy(x_test.reshape((-1,28*28) / 255))\n",
    "\n",
    "train_dataset = TensorDataset(x_train_tensor, x_train_tensor)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1000, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(x_test_tensor, x_test_tensor)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1000, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(torch.nn.Module):\n",
    "    def __init__(self, num_inputs, num_hidden):\n",
    "        super().__init__()\n",
    "         \n",
    "        # Building an linear encoder with Linear\n",
    "        # layer followed by Sigmoid activation function\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(num_inputs, num_hidden),\n",
    "            torch.nn.Sigmoid(),            \n",
    "        )\n",
    "         \n",
    "        # Building an linear decoder with Linear\n",
    "        # The Sigmoid activation function\n",
    "        # outputs the value between 0 and 1\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(num_hidden, num_inputs),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    " \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def continue_training():\n",
    "    network.train()      # configure the network for training\n",
    "    for i in range(10):  # train the network 10 times\n",
    "        epoch_loss = []\n",
    "        for data, target in train_loader:       # working in batchs of 1000\n",
    "            optimizer.zero_grad()               # initialize the learning system\n",
    "            output = network(data)              # feed in the data \n",
    "            loss = F.mse_loss(output, target)   # compute how wrong the output is\n",
    "            loss.backward()                     # change the weights to reduce error\n",
    "            optimizer.step()                    # update the learning rule\n",
    "            epoch_loss.append(loss.detach().numpy())\n",
    "            \n",
    "\n",
    "    # update the list of training accuracy values\n",
    "    training_loss.append(np.mean(epoch_loss)) # store the loss for later.\n",
    "    print('Iteration', len(training_loss), 'Training loss:', training_loss[-1])\n",
    "    \n",
    "    correct = 0\n",
    "    network.eval()\n",
    "    test_set_loss = []\n",
    "    for data, target in test_loader:    # go through the test data once (in groups of 1000)\n",
    "        output = network(data)                               # feed in the data\n",
    "        loss = F.mse_loss(output, target)\n",
    "        test_set_loss.append(loss.detach().numpy())\n",
    "        \n",
    "        \n",
    "    # update the list of testing accuracy values\n",
    "    testing_loss.append(np.mean(test_set_loss))\n",
    "    print('Iteration', len(testing_loss), 'Testing accuracy:', testing_loss[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1 [2 Marks]** For the autoencoder we will do the following:\n",
    "\n",
    "1. Train the network and plot the testing and training losses(repeated trials) for 5 different different values of the hidden layer size.  **Ensure the number of hidden layers is always less than the number of input features**.  Select a good number of training iterations (i.e., not overfitting) and a good number of hidden neurons\n",
    "2. Plot 10 input and reconstructed images from the training set and 10 from the testing set.  How do these compare to the reconstructions of from the Hopfield networks you constructed above?\n",
    "3. With your trained network, compare the loss on inputs corrupted salt and pepper noise.  Sweep through a range of number of pixels corrupted from 0 to 75\\%.  Because the image data is back in the range $[0,1]$, this time corrupt the image by setting pixels to equal 0 or 1 with 50\\% probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = ## Create the autoencoder here\n",
    "\n",
    "training_loss = []\n",
    "testing_loss = []\n",
    "\n",
    "# We're all hip, fashionable people here, let's use the Adam optimizer.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2 [1 Mark]** Described what you've observed about the results of the autoencoder, compared to the Hopfield networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.3 [1 Marks]** Sensory Preconditioning - Now we are going to look at small datasets. We are going to use the sensory preconditioning protocol, discussed by Gluck and Meyers.\n",
    "\n",
    "We will also need to construct a data set that performs the preconditioning.  We will break this down into three phases:\n",
    "\n",
    "1. Do the sensory preconditioning.  Plot the training loss vs number of epochs, describe the resultant behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 1: Train the network\n",
    "## features: s1, s2, context1, context2\n",
    "# we are teaching this network that these inputs only occur to gether, regardless of the context.\n",
    "stimuli = np.array([[0,0,0,0],\n",
    "                    [0,0,0,0],\n",
    "                    [0,0,1,0],\n",
    "                    [0,0,0,1],\n",
    "                    [0,0,1,1],\n",
    "                    [1,1,0,0],\n",
    "                    [1,1,0,0],\n",
    "                    [1,1,1,0],\n",
    "                    [1,1,0,1],\n",
    "                    [1,1,1,1]]).astype(np.float32)\n",
    "\n",
    "dataset = TensorDataset(torch.from_numpy(stimuli), torch.from_numpy(stimuli))\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "\n",
    "epochs = 2000\n",
    "outputs = []\n",
    "losses = []\n",
    "\n",
    "model = Autoencoder(4,3)# Your code here\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-1)\n",
    "loss_function = torch.nn.BCELoss()\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for (inp, out) in data_loader:\n",
    "        \n",
    "        # Output of Autoencoder\n",
    "        reconstructed = model(inp)\n",
    "    \n",
    "        # Calculating the loss function\n",
    "        loss = loss_function(reconstructed, out)\n",
    "        \n",
    "        # The gradients are set to zero,\n",
    "        # the gradient is computed and stored.\n",
    "        # .step() performs parameter update\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Storing the losses in a list for plotting\n",
    "        losses.append(loss.detach().numpy())\n",
    "        pass\n",
    "    outputs.append((epochs, inp, reconstructed))\n",
    "    pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss performance.\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Train the conditioned response to one stimulus.  We will train this using simple linear regression where we map the the hidden state to the desired output.  You can reuse the linear regression code from above.\n",
    "\n",
    "Lets call the features (columns) of the stimuli data as s1, s2, c1, and c2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "conditioning_stimuli = torch.from_numpy(np.array([[0,0,0,0],\n",
    "                    [0,0,1,0],\n",
    "                    [0,0,0,1],\n",
    "                    [0,0,1,1],\n",
    "                    [1,0,0,0],\n",
    "                    [1,0,1,0],\n",
    "                    [1,0,0,1],\n",
    "                    [1,0,1,1]]).astype(np.float32))\n",
    "conditioning_outputs = torch.from_numpy(np.array([[0,0,0,0,1,1,1,1]]).astype(np.float32).T)\n",
    "\n",
    "conditioning_features = model.encoder(conditioning_stimuli)\n",
    "\n",
    "def get_weights(features, values, lamb=0.01):\n",
    "    pass # Your code here\n",
    "\n",
    "def evaluate(weights, features):\n",
    "    pass # your code here\n",
    "\n",
    "W = get_weights(conditioning_features, conditioning_outputs)\n",
    "conditioned_response = evaluate(W, conditioning_features[4:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Report the strength of the conditioned response to the other stimulus.  Has the network transferred learning from one stimuls to the other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stimuli = torch.from_numpy(np.array([\n",
    "                    [0,1,0,0],\n",
    "                    [0,1,1,0],\n",
    "                    [0,1,0,1],\n",
    "                    [0,1,1,1]]).astype(np.float32))\n",
    "\n",
    "testing_features = model.encoder(test_stimuli)\n",
    "\n",
    "transferred_response = evaluate(W, testing_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Compare to the response of the system without either of the preconditioned inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unconditioned_responses = evaluate(W, conditioning_features[:4,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Plot graphs showing the strength of the response to the conditioned stimuli, the pre-conditioned stimuli, and the unconditioned stimuli\n",
    "(*i.e.*, s1 = 0, s2 = 0, and context in $\\{(0,0),(0,1),(1,0),(1,1)\\}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_mu = None # Your code here\n",
    "response_std = None # your code here\n",
    "\n",
    "ax.bar([1,2,3], response_mu, yerr=response_std, align='center', alpha=0.5, ecolor='black', capsize=10)\n",
    "ax.set_ylabel('Conditioned Responses')\n",
    "ax.set_xticks([1,2,3], labels=['s1+', 's2+', 's1-,s2-'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[BONUS]** [1 Mark] Repeat the sensory preconditioning, but use an conditioning stimulus that is not 100\\% correlated with the unconditioned stimulus. That is: change phase 2 so that it is not possible to perfectly predict the output."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
